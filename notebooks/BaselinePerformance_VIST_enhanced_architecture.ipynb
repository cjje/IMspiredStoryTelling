{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06e1efb-e140-4616-90b4-c0ab4715b915",
   "metadata": {},
   "source": [
    "# Questions to answer\n",
    "\n",
    "    Maybe attention is the problem!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977e0af1-910a-4092-808e-f3e92f8b75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer,VisionEncoderDecoderModel, ViTFeatureExtractor,ViTImageProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4432a9e-5267-4297-a544-288c80b9a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f340bf-2b05-4af2-a979-e19a01f87201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6d7a8-6f6a-4772-9c84-0718825b9beb",
   "metadata": {},
   "source": [
    "## 0. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cace07a6-2e20-4024-9553-5c38dd74bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "\n",
    "        try:\n",
    "            i_image = Image.open(image_path)\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "    \n",
    "        images.append(i_image)\n",
    "    \n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    print(pixel_values.shape)\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    \n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    print(output_ids)\n",
    "    \n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11b86285-b833-47b5-8326-1bc95de0906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step2(image_paths, encoder, decoder):\n",
    "    \n",
    "    preds_all = []\n",
    "    prev_pix = None\n",
    "    for idx, image_path in enumerate(image_paths):\n",
    "\n",
    "        try:\n",
    "            i_image = Image.open(image_path)\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        org_pixel_values = feature_extractor(images=i_image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        curr_pix = encoder(org_pixel_values).last_hidden_state\n",
    "        \n",
    "        if prev_pix != None:\n",
    "            pixel_values = torch.concat((prev_pix, curr_pix), 1)\n",
    "        else:\n",
    "            pixel_values = torch.concat((curr_pix, curr_pix), 1)\n",
    "        prev_pix = curr_pix\n",
    "            \n",
    "        pixel_values = pixel_values.to(device)\n",
    "        print(pixel_values.shape)\n",
    "        \n",
    "        output_ids = decoder.generate(pixel_values = pixel_values, **gen_kwargs)\n",
    "        print(output_ids)\n",
    "    \n",
    "        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "\n",
    "        print(preds)\n",
    "        preds_all.append(preds)\n",
    "            \n",
    "    return preds_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27789aad-0f83-4708-96aa-adc371bb6546",
   "metadata": {},
   "source": [
    "## 1. Load baseline model + basic performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5fe8cca-a8d0-4a30-a14f-cc40bbeee38b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d27609b-ecc7-4877-9ea4-bf2e65ce279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder embeddings\n",
      "encoder encoder\n",
      "encoder layernorm\n",
      "encoder pooler\n",
      "decoder transformer\n",
      "decoder lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, child in model.named_children():\n",
    "        for x, y in child.named_children():\n",
    "            print(name,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd26f8a9-ef20-462b-b60f-4115bd09e35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0edde92-e581-43c0-980f-d0f80bd4e1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe8a66f-0b27-4fd6-baed-a4e3242ee5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0d0501-0bd0-4f1a-9ed0-d671ee028117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='nlpconnect/vit-gpt2-image-captioning', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f8928-cfde-46f6-970b-16b3941b0eeb",
   "metadata": {},
   "source": [
    "## 2. Load VIST images\n",
    "\n",
    "    Loading is slightly different from the previous notebook, as the entire set of images for one story together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a980a54-fa99-4ebd-8507-d949d2117691",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ed0cd5-e2ef-4c99-af31-8e73dc4dc5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_ids': ['80', '81', '82', '83', '84'],\n",
       " 'img_ids': ['181647714', '181626113', '181645575', '181635518', '181640606'],\n",
       " 'album_id': '72157594187037689',\n",
       " 'text': ['we took a nice hike into the forest today .',\n",
       "  'we were lucky enough to see some wildlife , like this deer .',\n",
       "  'this guy was friendly . he must hit up all the hikers for food .',\n",
       "  \"i 'm glad we spotted this snake before we got too close !\",\n",
       "  'the end of our hike rewarded us with an amazing view of the falls !']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = os.listdir(\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/annotations\")\n",
    "labels = open(\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/annotations/StoriesFin.json\")\n",
    "labels = json.load(labels)\n",
    "labels[\"16\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ef8fd6-6f0c-47e6-bd6d-2d64400b1327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5149"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2069e4f9-86b4-4f6a-bd71-9b1958cfe3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_ids': ['80', '81', '82', '83', '84'],\n",
       " 'img_ids': ['181647714', '181626113', '181645575', '181635518', '181640606'],\n",
       " 'album_id': '72157594187037689',\n",
       " 'text': ['we took a nice hike into the forest today .',\n",
       "  'we were lucky enough to see some wildlife , like this deer .',\n",
       "  'this guy was friendly . he must hit up all the hikers for food .',\n",
       "  \"i 'm glad we spotted this snake before we got too close !\",\n",
       "  'the end of our hike rewarded us with an amazing view of the falls !']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "905f68bc-71d4-4c51-a99c-7cb4eb6c9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories = random.sample(list(labels.keys()),100) # choose 100 random stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fa6a14e-21cf-4ec4-9663-2fa913e89884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_ids': ['80', '81', '82', '83', '84'],\n",
       " 'img_ids': ['181647714', '181626113', '181645575', '181635518', '181640606'],\n",
       " 'album_id': '72157594187037689',\n",
       " 'text': ['we took a nice hike into the forest today .',\n",
       "  'we were lucky enough to see some wildlife , like this deer .',\n",
       "  'this guy was friendly . he must hit up all the hikers for food .',\n",
       "  \"i 'm glad we spotted this snake before we got too close !\",\n",
       "  'the end of our hike rewarded us with an amazing view of the falls !']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a440a4-a88f-4819-b623-6ad0b85e59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "for s in labels:\n",
    "    for idx, im in enumerate(labels[s]['img_ids']):\n",
    "        image_path = f\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/{im}.jpeg\"\n",
    "        key = f\"story_{s}\"\n",
    "        if key not in train:\n",
    "            train[key] = {}\n",
    "        train[key][idx] = {}\n",
    "        train[key][idx]['image_path'] = image_path\n",
    "        train[key][idx]['text'] = labels[s]['text'][idx]\n",
    "\n",
    "    # train[s] = {}\n",
    "    # train[s][\"image_paths\"] = []\n",
    "    # train[s][\"img_ids\"] = labels[s]['img_ids']\n",
    "    # train[s][\"text\"] = labels[s]['text']    \n",
    "    # for im in train[s][\"img_ids\"]:\n",
    "    #     train[s][\"image_paths\"].append(f\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/{im}.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f2450ff-e39c-487e-9325-f26aeb0aa540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181647714.jpeg',\n",
       "  'text': 'we took a nice hike into the forest today .'},\n",
       " 1: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181626113.jpeg',\n",
       "  'text': 'we were lucky enough to see some wildlife , like this deer .'},\n",
       " 2: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181645575.jpeg',\n",
       "  'text': 'this guy was friendly . he must hit up all the hikers for food .'},\n",
       " 3: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181635518.jpeg',\n",
       "  'text': \"i 'm glad we spotted this snake before we got too close !\"},\n",
       " 4: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181640606.jpeg',\n",
       "  'text': 'the end of our hike rewarded us with an amazing view of the falls !'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['story_16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d21783b2-510d-460a-804f-5a6c41872e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5149"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9387bb42-fab0-415e-830c-990ae1ddab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story_19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181647714.jpeg',\n",
       "  'text': 'giant sequoia tree and red woods in the forest .'},\n",
       " 1: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181626113.jpeg',\n",
       "  'text': 'a young deer scampers about in the woods .'},\n",
       " 2: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181645575.jpeg',\n",
       "  'text': 'grey squirrel holding some food with his paws .'},\n",
       " 3: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181635518.jpeg',\n",
       "  'text': 'the snake slithers quietly through the underbrush .'},\n",
       " 4: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181640606.jpeg',\n",
       "  'text': 'beautiful picture taken of a river running through the valley .'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(train.keys())[2])\n",
    "train[list(train.keys())[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001d05f4-1bc1-4296-9010-9f138cd601e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/181640606.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfb511-2ddc-41ca-911f-17aa430587da",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d39a9eb-7c58-45df-bb29-da5bb3b48872",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_test = os.listdir(\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/annotations\")\n",
    "labels_test = open(\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/annotations/TestStoriesFin.json\")\n",
    "labels_test = json.load(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87947dbd-6a34-40a8-9863-ae74835ec86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_ids': ['227655', '227656', '227657', '227658', '227659'],\n",
       " 'img_ids': ['1741625', '1741640', '1741639', '1741633', '1741630'],\n",
       " 'album_id': '44277',\n",
       " 'text': ['i was so excited to be heading to the crafts fair .',\n",
       "  'when i arrived i saw a great booth with a variety of great crafts .',\n",
       "  \"i stopped at chatted at my friend [female] 's booth for a bit .\",\n",
       "  'there were even booths set up for all of the kids .',\n",
       "  \"i found some awesome crafts at the fair , i 'm really happy that i went .\"]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test['45531']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d437946-0657-41f1-bfba-51f6a0a41e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {}\n",
    "for s in labels_test:\n",
    "    for idx, im in enumerate(labels_test[s]['img_ids']):\n",
    "        image_path = f\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/{im}.jpeg\"\n",
    "        key = f\"story_{s}\"\n",
    "        if key not in test:\n",
    "            test[key] = {}\n",
    "        test[key][idx] = {}\n",
    "        test[key][idx]['image_path'] = image_path\n",
    "        test[key][idx]['text'] = labels_test[s]['text'][idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93d22b9c-ee20-4d37-8d78-5742c67648fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2269"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a820245-9b67-456e-93b7-1592927da9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story_45531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741625.jpeg',\n",
       "  'text': 'i was so excited to be heading to the crafts fair .'},\n",
       " 1: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741640.jpeg',\n",
       "  'text': 'when i arrived i saw a great booth with a variety of great crafts .'},\n",
       " 2: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741639.jpeg',\n",
       "  'text': \"i stopped at chatted at my friend [female] 's booth for a bit .\"},\n",
       " 3: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741633.jpeg',\n",
       "  'text': 'there were even booths set up for all of the kids .'},\n",
       " 4: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741630.jpeg',\n",
       "  'text': \"i found some awesome crafts at the fair , i 'm really happy that i went .\"}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(test.keys())[0])\n",
    "test[list(test.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1857c4-c51e-4512-8256-ff148f5551b3",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e75d016e-636d-44b7-8c44-429240c3d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_val = os.listdir(\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/val/annotations\")\n",
    "labels_val = open(\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/val/annotations/ValStoriesFin.json\")\n",
    "labels_val = json.load(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e94f6aef-6e98-4610-b068-9922fa81bed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_ids': ['202350', '202351', '202352', '202353', '202354'],\n",
       " 'img_ids': ['693397887', '695160730', '694227508', '693397865', '694227468'],\n",
       " 'album_id': '72157600601428727',\n",
       " 'text': ['my sister arrived early to help me with the family bar bq .',\n",
       "  'every one else arrived soon after .',\n",
       "  'dad manned the grill .',\n",
       "  'there was so much food and it was all delicious .',\n",
       "  'we ended the day shooting off some fireworks .']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_val[list(labels_val.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c02fe38-b0a6-419e-8b73-f139e9d5db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = {}\n",
    "for s in labels_val:\n",
    "    for idx, im in enumerate(labels_val[s]['img_ids']):\n",
    "        image_path = f\"/home/jay.je/IMspiredStoryTelling/datasets/VIST/val/images/{im}.jpeg\"\n",
    "        key = f\"story_{s}\"\n",
    "        if key not in val:\n",
    "            val[key] = {}\n",
    "        val[key][idx] = {}\n",
    "        val[key][idx]['image_path'] = image_path\n",
    "        val[key][idx]['text'] = labels_val[s]['text'][idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f46bce39-93c4-4531-886c-fbe8b9239462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2223"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98685c1-f24b-48f1-a37f-c453017609dd",
   "metadata": {},
   "source": [
    "# 2. Fine-tune on the VIST dataset\n",
    "\n",
    "    Actually, I think we do not need to fine-tune the feature extractor.\n",
    "    Just fine-tune the decoder based on the features produced by the extractor <<\n",
    "\n",
    "\n",
    "    SOMEHOW have the dataloader load 5 images (so it's 5 x 3 x H x W)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34a2a8f7-ed02-4fb5-9110-a7ca9775b47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/258058601.jpeg',\n",
       "  'text': 'the party has many pictures .'},\n",
       " 1: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/258058806.jpeg',\n",
       "  'text': 'the guys meet [male] .'},\n",
       " 2: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/258059074.jpeg',\n",
       "  'text': 'they have subway .'},\n",
       " 3: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/258059285.jpeg',\n",
       "  'text': 'they go to the museum .'},\n",
       " 4: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/258059887.jpeg',\n",
       "  'text': 'they are flying back home .'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['story_26929']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c5a6b18-9ef5-43cf-ad17-4668941b58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, stories_annotations, img_dir=None, transform=None, target_transform=None):\n",
    "        # self.img_labels = annotations_file\n",
    "        # self.img_dir = img_dir\n",
    "        self.stories = stories_annotations\n",
    "        self.stories_keys = list(stories_annotations.keys())\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in our dataset\n",
    "        \"\"\"\n",
    "        return len(list(self.stories.keys()))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if idx >= self.__len__():\n",
    "            return\n",
    "        story_num = self.stories_keys[idx]\n",
    "        \n",
    "        # get label\n",
    "        # label = self.stories[story_num]['text']\n",
    "        # label = tokenizer(label, padding=\"max_length\").input_ids # omitted max_target_lengths\n",
    "        # label = torch.tensor(label).squeeze()\n",
    "        inputs = {}\n",
    "        imgs = []\n",
    "        labs = []\n",
    "        atts = []\n",
    "\n",
    "        try:\n",
    "            prev_img = None\n",
    "            \n",
    "            for seq in self.stories[story_num]:\n",
    "                img_source = self.stories[story_num][seq]['image_path']\n",
    "                img = read_image(img_source)\n",
    "                img = feature_extractor(images=img, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "                imgs.append(img.clone().detach())\n",
    "                \n",
    "                lab_source = self.stories[story_num][seq]['text']\n",
    "                tok_out = tokenizer(lab_source, padding=\"max_length\")\n",
    "                lab = tok_out.input_ids\n",
    "                lab = torch.tensor(lab).squeeze()\n",
    "                labs.append(lab.clone().detach())\n",
    "\n",
    "                # att = tok_out.attention_mask\n",
    "                # att = torch.tensor(att).squeeze()\n",
    "                # atts.append(att.clone().detach())\n",
    "\n",
    "                \n",
    "            inputs['pixel_values'] = torch.stack(imgs)\n",
    "            inputs['labels'] = torch.stack(labs)\n",
    "            # inputs['attention_mask'] = torch.stack(atts)\n",
    "            \n",
    "        except:\n",
    "            return self.__getitem__(idx+1)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24f4efd3-8731-440e-8073-aca803634017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5149\n",
      "2223\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e59f3f5-5274-476b-839f-ccd240d228e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = random.sample(list(train.keys()), 1000)\n",
    "val_idx = random.sample(list(val.keys()), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "066e78a3-5111-46f3-973b-a5f3b778655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin = {}\n",
    "for k in train_idx:\n",
    "    train_fin[k] = train[k]\n",
    "\n",
    "val_fin = {}\n",
    "for k in val_idx:\n",
    "    val_fin[k] = val[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "945368ea-d3c0-4a2f-aa5e-3f1a622af71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_train_data = CustomImageDataset(train_fin)\n",
    "train_dataloader = DataLoader(custom_train_data, batch_size=1, shuffle=True) # , collate_fn=my_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b719f7b-65f0-47f3-9624-e596dc1e42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_val_data = CustomImageDataset(val_fin)\n",
    "val_dataloader = DataLoader(custom_val_data, batch_size=1, shuffle=True) # , collate_fn=my_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d20c84c0-3194-4353-8391-c66616e72b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 3, 224, 224])\n",
      "torch.Size([1, 5, 1024])\n"
     ]
    }
   ],
   "source": [
    "output = next(iter(train_dataloader))\n",
    "print(output['pixel_values'].shape)\n",
    "print(output['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a0be7db-72c8-462f-8597-48cdda67ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 3, 224, 224])\n",
      "torch.Size([1, 5, 1024])\n"
     ]
    }
   ],
   "source": [
    "output = next(iter(val_dataloader))\n",
    "print(output['pixel_values'].shape)\n",
    "print(output['labels'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2ba47-d757-4969-bf49-0ac7403c72ca",
   "metadata": {},
   "source": [
    "## Now try to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acf1b252-00aa-4d3f-bd5e-32cd8cabd02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a8e0ce7-e776-4f40-9da7-f3a3e74cc6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fba0c8ff-ea63-4542-b52b-530a1b58c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_pad_token_for_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "314958d1-7aaf-4111-875b-60e582696c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4aaacdb3-6b92-4e19-ac94-cf25dbfc8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer = None):\n",
    "    \"\"\"\n",
    "    Idea 1: The next sample gets the previously generated model output in the model decoder part only (previous sentence)\n",
    "    Idea 2: The next sample gets the previously generated feature extractor (previous image)\n",
    "    \"\"\"\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs = data\n",
    "\n",
    "        # for each image, can iterate!\n",
    "        # eg shape: (1, 5, 3, H, W), (1, 5, 2056)\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        last_pix = None\n",
    "        for k in range(5):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pix = inputs['pixel_values'][0][k].unsqueeze(0).to(device)\n",
    "            lab = inputs['labels'][0][k].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            curr_pix = encoder(pix, return_dict=True).last_hidden_state\n",
    "            if last_pix != None:\n",
    "                pix = torch.concat((last_pix, curr_pix), 1)\n",
    "            else:\n",
    "                pix = torch.concat((curr_pix, curr_pix), 1)\n",
    "            last_pix = curr_pix.detach()\n",
    "\n",
    "            outputs = decoder(input_ids = lab, labels = lab, encoder_hidden_states = pix, output_hidden_states = True\n",
    "                             )\n",
    "            # Compute the loss and its gradients\n",
    "            loss = outputs.loss\n",
    "            # loss = loss_fn(outputs, inputs['labels'])\n",
    "            loss.backward()\n",
    "    \n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            last_loss = running_loss / (100) # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd3a143a-5957-4fa1-8b59-52ade928e63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a78a0be0-f17c-4946-a8d1-2d538a76f22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = model.base_model.encoder\n",
    "decoder = model.base_model.decoder\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a7a03ac-725d-4885-8b6f-d6de8bc0fbfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1 loss: 0.038397381529211995\n",
      "  batch 101 loss: 0.4802251105569303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 6299 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 5957 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 7571 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 0.4178759583551437\n",
      "  batch 301 loss: 0.39035042848438023\n",
      "  batch 401 loss: 0.3832549136132002\n",
      "  batch 501 loss: 0.3683503209985793\n",
      "  batch 601 loss: 0.3890676255710423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 701 loss: 0.35347841528244317\n",
      "  batch 801 loss: 0.34859455096535386\n",
      "  batch 901 loss: 0.3511508968565613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1 loss: 0.0023934620060026644\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n",
      "LOSS train 0.3511508968565613 valid 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     voutputs \u001b[38;5;241m=\u001b[39m decoder(input_ids \u001b[38;5;241m=\u001b[39m lab, labels \u001b[38;5;241m=\u001b[39m lab, encoder_hidden_states \u001b[38;5;241m=\u001b[39m pix, output_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     39\u001b[0m                       \u001b[38;5;66;03m# attention_mask = att\u001b[39;00m\n\u001b[1;32m     40\u001b[0m                      )\n\u001b[1;32m     42\u001b[0m     vloss \u001b[38;5;241m=\u001b[39m voutputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 43\u001b[0m     vrunning_vloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mvloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     46\u001b[0m     last_loss \u001b[38;5;241m=\u001b[39m vrunning_vloss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m# loss per batch\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000\n",
    "\n",
    "optimizer = torch.optim.SGD(decoder.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    decoder.train()\n",
    "    avg_loss = train_one_epoch(epoch_number) #writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    # print(\"Got to eval\")\n",
    "    decoder.eval()\n",
    "    vrunning_vloss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "  \n",
    "            vinputs = vdata\n",
    "            last_pix = None\n",
    "            \n",
    "            for k in range(5):\n",
    "                pix = vinputs['pixel_values'][0][k].unsqueeze(0).to(device)\n",
    "                lab = vinputs['labels'][0][k].unsqueeze(0).to(device)\n",
    "                \n",
    "                # Make predictions for this batch\n",
    "                curr_pix = encoder(pix, return_dict=True).last_hidden_state\n",
    "                if last_pix != None:\n",
    "                    pix = torch.concat((last_pix, curr_pix), 1)\n",
    "                else:\n",
    "                    pix = torch.concat((curr_pix, curr_pix), 1)\n",
    "                last_pix = curr_pix.detach()\n",
    "                \n",
    "                voutputs = decoder(input_ids = lab, labels = lab, encoder_hidden_states = pix, output_hidden_states = True\n",
    "                                  # attention_mask = att\n",
    "                                 )\n",
    "\n",
    "                vloss = voutputs.loss\n",
    "                vrunning_vloss += vloss.item()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                last_loss = vrunning_vloss / (100) # loss per batch\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                vrunning_vloss = 0.\n",
    "    \n",
    "            avg_vloss = running_vloss / (i + 1)\n",
    "            print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    print(f\"At epoch {epoch} because avg loss was {avg_vloss} and best loss was {best_vloss}\")\n",
    "    if avg_vloss < best_vloss:\n",
    "        # print(f\"Saving...\")\n",
    "        # encoder.save_pretrained(f'./model_epoch{epoch}_encoder_VIST_5000/')\n",
    "        # decoder.save_pretrained(f'./model_epoch{epoch}_decoder_2imgs_VIST_5000/')\n",
    "        # best_vloss = avg_vloss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9873c4e3-4138-4702-a0b1-658c05eb13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test['story_45531']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7a1397a5-1366-4f0a-a266-f7a7c33f13df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741625.jpeg',\n",
       "  'text': 'i was so excited to be heading to the crafts fair .'},\n",
       " 1: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741640.jpeg',\n",
       "  'text': 'when i arrived i saw a great booth with a variety of great crafts .'},\n",
       " 2: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741639.jpeg',\n",
       "  'text': \"i stopped at chatted at my friend [female] 's booth for a bit .\"},\n",
       " 3: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741633.jpeg',\n",
       "  'text': 'there were even booths set up for all of the kids .'},\n",
       " 4: {'image_path': '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741630.jpeg',\n",
       "  'text': \"i found some awesome crafts at the fair , i 'm really happy that i went .\"}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dc8923af-aa1f-47c3-ac5d-e6188794bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for k in sample:\n",
    "    imgs.append(sample[k]['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e203db5-2e65-448d-b7a8-b8c4f2133700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741625.jpeg',\n",
       " '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741640.jpeg',\n",
       " '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741639.jpeg',\n",
       " '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741633.jpeg',\n",
       " '/home/jay.je/IMspiredStoryTelling/datasets/VIST/test/images/1741630.jpeg']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dd0f7fa8-6e88-4f0a-a7a8-6f8ad663431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = VisionEncoderDecoderModel(encoder = encoder, decoder = decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07f989cf-2dab-467b-8766-99557f9085e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_generation(model, custom_loader):\n",
    "\n",
    "    ls_out = []\n",
    "\n",
    "    output = next(iter(custom_loader))\n",
    "\n",
    "    prev_enc = None\n",
    "    for i in range(5):\n",
    "        pix = output['pixel_values'][0][i].unsqueeze(0).to(device)\n",
    "        # att = output['attention_mask'][0][i].unsqueeze(0).to(device)\n",
    "\n",
    "        print(f\"{i}th image\")\n",
    "        print(pix.shape)\n",
    "        # print(att.shape)\n",
    "\n",
    "        enc_out = model.encoder(pix)\n",
    "        \n",
    "        if prev_enc != None:\n",
    "            enc_out.last_hidden_state = torch.concat((prev_enc, enc_out.last_hidden_state),1)\n",
    "        \n",
    "        else:\n",
    "            enc_out.last_hidden_state = torch.concat((enc_out.last_hidden_state, enc_out.last_hidden_state),1)\n",
    "\n",
    "        print(\"at prev enc stage\")\n",
    "        prev_enc = enc_out.last_hidden_state\n",
    "\n",
    "        print(\"outout state\")\n",
    "        outputids = model.generate(encoder_outputs=enc_out, **gen_kwargs)# attention_mask = att,\n",
    "\n",
    "        print(tokenizer.batch_decode(outputids))\n",
    "        ls_out.append(outputids) \n",
    "    \n",
    "    return ls_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "13d500e6-da63-4282-8965-893ead7f247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th image\n",
      "torch.Size([1, 3, 224, 224])\n",
      "at prev enc stage\n",
      "outout state\n",
      "['<|endoftext|>.<|endoftext|>']\n",
      "1th image\n",
      "torch.Size([1, 3, 224, 224])\n",
      "at prev enc stage\n",
      "outout state\n",
      "['<|endoftext|>.<|endoftext|>']\n",
      "2th image\n",
      "torch.Size([1, 3, 224, 224])\n",
      "at prev enc stage\n",
      "outout state\n",
      "['<|endoftext|>.<|endoftext|>']\n",
      "3th image\n",
      "torch.Size([1, 3, 224, 224])\n",
      "at prev enc stage\n",
      "outout state\n",
      "['<|endoftext|>.<|endoftext|>']\n",
      "4th image\n",
      "torch.Size([1, 3, 224, 224])\n",
      "at prev enc stage\n",
      "outout state\n",
      "['<|endoftext|>.<|endoftext|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[50256,   764, 50256]], device='cuda:0'),\n",
       " tensor([[50256,   764, 50256]], device='cuda:0'),\n",
       " tensor([[50256,   764, 50256]], device='cuda:0'),\n",
       " tensor([[50256,   764, 50256]], device='cuda:0'),\n",
       " tensor([[50256,   764, 50256]], device='cuda:0')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_generation(model_test, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b3b5b-503b-4bae-a1a1-da5779d2e338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b99f3-19d7-4250-8298-6332a191fe2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cce04912-c67f-4090-a2e7-ea201ecb97bf",
   "metadata": {},
   "source": [
    "## See model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cd48b9fd-d1f0-49c6-bf2b-752e097e0ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GenerationMixin.generate of GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (crossattention): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (q_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.generate # decoder has generate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a32b77c-2a88-4c49-97e8-e3601874628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = labels['20262']\n",
    "sample['image_paths'] = []\n",
    "for im in sample['img_ids']:\n",
    "    sample['image_paths'].append(f'/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/{im}.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "acecea0f-8f64-4192-bca5-cd1a44e1cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step2(image_paths):\n",
    "    \n",
    "    preds_all = []\n",
    "    prev_pix = None\n",
    "    for idx, image_path in enumerate(image_paths):\n",
    "\n",
    "        try:\n",
    "            i_image = Image.open(image_path)\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        org_pixel_values = feature_extractor(images=i_image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        curr_pix = encoder(org_pixel_values).last_hidden_state\n",
    "        \n",
    "        if prev_pix != None:\n",
    "            pixel_values = torch.concat((prev_pix, curr_pix), 1)\n",
    "        else:\n",
    "            pixel_values = torch.concat((curr_pix, curr_pix), 1)\n",
    "        prev_pix = curr_pix\n",
    "            \n",
    "        pixel_values = pixel_values.to(device)\n",
    "        print(pixel_values.shape)\n",
    "        \n",
    "        output_ids = decoder.generate(pixel_values = pixel_values, **gen_kwargs)\n",
    "        print(output_ids)\n",
    "    \n",
    "        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "\n",
    "        print(preds)\n",
    "        preds_all.append(preds)\n",
    "            \n",
    "    return preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "93ef46c2-602a-4406-94d2-cdf62ca4d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 394, 768])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['pixel_values'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict_step2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_paths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# train images\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[118], line 28\u001b[0m, in \u001b[0;36mpredict_step2\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m     25\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(pixel_values\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 28\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_ids)\n\u001b[1;32m     31\u001b[0m preds \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1433\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1433\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1249\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1252\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['pixel_values'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "predict_step2(sample['image_paths']) # train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30a5a5-14bf-44f7-a89e-a5b5419e920b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b556feac-38b4-4532-bd0a-f8a1a5772d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_ids': ['227655', '227656', '227657', '227658', '227659'],\n",
       " 'img_ids': ['1741625', '1741640', '1741639', '1741633', '1741630'],\n",
       " 'album_id': '44277',\n",
       " 'text': ['i was so excited to be heading to the crafts fair .',\n",
       "  'when i arrived i saw a great booth with a variety of great crafts .',\n",
       "  \"i stopped at chatted at my friend [female] 's booth for a bit .\",\n",
       "  'there were even booths set up for all of the kids .',\n",
       "  \"i found some awesome crafts at the fair , i 'm really happy that i went .\"]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test['45531']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47d36410-3a74-4068-bec9-83e1bb38bfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_ids': ['227655', '227656', '227657', '227658', '227659'],\n",
       " 'img_ids': ['1741625', '1741640', '1741639', '1741633', '1741630'],\n",
       " 'album_id': '44277',\n",
       " 'text': ['i was so excited to be heading to the crafts fair .',\n",
       "  'when i arrived i saw a great booth with a variety of great crafts .',\n",
       "  \"i stopped at chatted at my friend [female] 's booth for a bit .\",\n",
       "  'there were even booths set up for all of the kids .',\n",
       "  \"i found some awesome crafts at the fair , i 'm really happy that i went .\"]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e6f5ebd-1754-412f-9f09-e4e0f05ffdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['image_paths'] = []\n",
    "for im in sample['img_ids']:\n",
    "    sample['image_paths'].append(f'/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/{im}.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "40c8c77d-c199-45fb-a9a7-e97a1f6e8315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_ids': ['101310', '101311', '101312', '101313', '101314'],\n",
       " 'img_ids': ['6631123887',\n",
       "  '6631122551',\n",
       "  '6631123221',\n",
       "  '6643755711',\n",
       "  '6631124129'],\n",
       " 'album_id': '72157628706301801',\n",
       " 'text': ['my boyfriend is a great guy . he decided to take me on a tour of our city to see the sights . this is a picture of him at the start .',\n",
       "  'our first stop was to this old antique store . out front was this little toy . my favorite animal a tiger !',\n",
       "  'walking further on , we came across some amazing street music . a dance or two later and we were still having out with them .',\n",
       "  'leaving the music behind we headed south . the most beautiful church loomed in the distance . the day was coming to an end and started to become chilly .',\n",
       "  'our solution , was coffee !'],\n",
       " 'image_paths': ['/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/6631123887.jpeg',\n",
       "  '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/6631122551.jpeg',\n",
       "  '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/6631123221.jpeg',\n",
       "  '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/6643755711.jpeg',\n",
       "  '/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/6631124129.jpeg']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "dace2b39-29ce-40ff-9f0d-afb96f5de029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('/home/jay.je/IMspiredStoryTelling/datasets/VIST/train/images/6631123887.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2a7d6849-ee60-481e-99b1-7cf876b53925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764]],\n",
      "       device='cuda:0')\n",
      "['...................................................................................................']\n",
      "tensor([[50256,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764]],\n",
      "       device='cuda:0')\n",
      "['...................................................................................................']\n",
      "tensor([[50256,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764, 50256]],\n",
      "       device='cuda:0')\n",
      "['.......................................................................................']\n",
      "tensor([[50256,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764, 50256]],\n",
      "       device='cuda:0')\n",
      "['......................................................................................']\n",
      "tensor([[50256,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764,   764,   764,\n",
      "           764,   764,   764,   764,   764,   764,   764,   764, 50256]],\n",
      "       device='cuda:0')\n",
      "['.......................................................................................']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['...................................................................................................'],\n",
       " ['...................................................................................................'],\n",
       " ['.......................................................................................'],\n",
       " ['......................................................................................'],\n",
       " ['.......................................................................................']]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step2(sample['image_paths']) # train images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aee069-7421-45fe-83e5-98dd0efd0bbf",
   "metadata": {},
   "source": [
    "Previous output \n",
    "\n",
    "    ['a man with a beard is looking at the camera',\n",
    "     'a toy bear sitting on top of a box',\n",
    "     'a crowd of people standing in front of a building',\n",
    "     'a large building with a clock on the front of it',\n",
    "     'a man holding a cup of coffee in front of his face']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67baf0e5-25b9-45f7-be0c-c1d11c8a27d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,   732,   389,  2045,   379,   281,  1468,  4590,   286,   257,\n",
      "          1448,   286,  4695,   287,   257,  2214,   286,  8701,   290,  7150,\n",
      "           764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256,   732,   460,   766,   326,   612,   373,   257,  1256,   286,\n",
      "          3404,   287,   262,  4286,   764, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256,   732,   389,  2045,   379,   257,  2415,   508,   318, 12049,\n",
      "           287,   257, 16569,   764,   764,   764,   764, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256,   732,   766,   257, 12036,   286,   257,  2415,   287,   257,\n",
      "         45810,   290,   257, 12036,   286,   257,  2415,   287,   257, 45810,\n",
      "           290,   257, 12036,   286,   257,  2415,   287,   257, 45810,   290,\n",
      "           257, 12036,   286,   257,  2415,   287,   257, 45810,   290,   257,\n",
      "         12036,   286,   257,  2415,   287,   257, 45810,   290,   257, 12036,\n",
      "           286,   257,  2415,   287,   257, 45810,   290,   257, 12036,   286,\n",
      "           257,  2415,   287,   257, 45810,   290,   257, 12036,   286,   257,\n",
      "          2415,   287,   257, 45810,   290,   257, 12036,   286,   257,  2415,\n",
      "           287,   257, 45810,   290,   257, 12036,   286,   257,  2415,   287,\n",
      "           257, 45810,   290,   257, 12036,   286,   257,  2415,   287,   257],\n",
      "        [50256,   732,   389,  2045,   379,   257, 12036,   286,   257,  1448,\n",
      "           286,  4695,   764, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['we are looking at an old photo of a group of animals in a field of grass and trees.',\n",
       " 'we can see that there was a lot of stuff in the picture.',\n",
       " 'we are looking at a woman who is dressed in a costume....',\n",
       " 'we see a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a bikini and a painting of a woman in a',\n",
       " 'we are looking at a painting of a group of animals.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(c_test['story_13']['image_paths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65df72-1b36-43dc-952c-114d4f4246d8",
   "metadata": {},
   "source": [
    "Original input\n",
    "\n",
    "    ['a painting of a castle with a bunch of animals on top of it',\n",
    "     'a close up image of a close up image of a bird',\n",
    "     'a painting of a woman holding a pink umbrella',\n",
    "     'a painting of a woman in a bathing suit',\n",
    "     'a painting of a cartoon character on a wall']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
